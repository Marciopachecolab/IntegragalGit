#!/usr/bin/env python3
"""
Safe mojibake scanner/fixer for the repository.

Behavior:
- Reads text files; tries UTF-8 first, falls back to Latin-1 on decode error.
- Detects common mojibake tokens (Ã¡, Ã£, Ã©, â€“, â€”, ", â€, etc.).
- Applies a replacement map to restore accents/punctuation.
- Rewrites only when content changes; saves as UTF-8 (no BOM).

Usage:
    python fix_encoding_safe.py [root_dir]
Defaults to the directory where the script lives.
"""

from __future__ import annotations

import sys
from pathlib import Path
from typing import Iterable

# Replacement map: mojibake token -> correct character
MOJIBAKE_MAP: dict[str, str] = {
    # lowercase
    "\u00c3\u00a1": "\u00e1",  # Ã¡ -> Ã¡
    "\u00c3\u00a2": "\u00e2",  # Ã¢ -> Ã¢
    "\u00c3\u00a3": "\u00e3",  # Ã£ -> Ã£
    "\u00c3\u00a4": "\u00e4",  # Ã¤ -> Ã¤
    "\u00c3\u00a0": "\u00e0",  # Ã  -> Ã 
    "\u00c3\u00aa": "\u00ea",  # Ãª -> Ãª
    "\u00c3\u00a9": "\u00e9",  # Ã© -> Ã©
    "\u00c3\u00a8": "\u00e8",  # Ã¨ -> Ã¨
    "\u00c3\u00ab": "\u00eb",  # Ã« -> Ã«
    "\u00c3\u00ae": "\u00ee",  # Ã® -> Ã®
    "\u00c3\u00ad": "\u00ed",  # Ã­ -> Ã­
    "\u00c3\u00ac": "\u00ec",  # Ã¬ -> Ã¬
    "\u00c3\u00af": "\u00ef",  # Ã¯ -> Ã¯
    "\u00c3\u00b4": "\u00f4",  # Ã´ -> Ã´
    "\u00c3\u00b3": "\u00f3",  # Ã³ -> Ã³
    "\u00c3\u00b2": "\u00f2",  # Ã² -> Ã²
    "\u00c3\u00b6": "\u00f6",  # Ã¶ -> Ã¶
    "\u00c3\u00b5": "\u00f5",  # Ãµ -> Ãµ
    "\u00c3\u00bc": "\u00fc",  # Ã¼ -> Ã¼
    "\u00c3\u00ba": "\u00fa",  # Ãº -> Ãº
    "\u00c3\u00b9": "\u00f9",  # Ã¹ -> Ã¹
    "\u00c3\u00a7": "\u00e7",  # Ã§ -> Ã§
    "\u00c3\u00b1": "\u00f1",  # Ã± -> Ã±
    # uppercase
    "\u00c3\u0081": "\u00c1",  # Ãƒï¿½ -> Ã
    "\u00c3\u0082": "\u00c2",  # Ã‚ -> Ã‚
    "\u00c3\u0083": "\u00c3",  # Ãƒ -> Ãƒ
    "\u00c3\u0084": "\u00c4",  # Ãƒâ€ž -> Ã„
    "\u00c3\u0080": "\u00c0",  # Ã€ -> Ã€
    "\u00c3\u008a": "\u00ca",  # ÃŠ -> ÃŠ
    "\u00c3\u0089": "\u00c9",  # Ã‰ -> Ã‰
    "\u00c3\u0088": "\u00c8",  # ÃƒË† -> Ãˆ
    "\u00c3\u008b": "\u00cb",  # Ãƒâ€¹ -> Ã‹
    "\u00c3\u008e": "\u00ce",  # ÃŽ -> ÃŽ
    "\u00c3\u008d": "\u00cd",  # Ãƒï¿½ -> Ã
    "\u00c3\u008c": "\u00cc",  # ÃƒÅ’ -> ÃŒ
    "\u00c3\u008f": "\u00cf",  # Ãƒï¿½ -> Ã
    "\u00c3\u0094": "\u00d4",  # Ã” -> Ã”
    "\u00c3\u0093": "\u00d3",  # Ã“ -> Ã“
    "\u00c3\u0092": "\u00d2",  # Ãƒâ€™ -> Ã’
    "\u00c3\u0096": "\u00d6",  # Ãƒâ€“ -> Ã–
    "\u00c3\u0095": "\u00d5",  # Ã• -> Ã•
    "\u00c3\u009c": "\u00dc",  # ÃƒÅ“ -> Ãœ
    "\u00c3\u009a": "\u00da",  # Ãš -> Ãš
    "\u00c3\u0099": "\u00d9",  # Ãƒâ„¢ -> Ã™
    "\u00c3\u0087": "\u00c7",  # Ã‡ -> Ã‡
    "\u00c3\u0091": "\u00d1",  # Ãƒ,
    'â€™':  -> Ã‘
    # punctuation / symbols
    "\u00e2\u20ac\u201c": "\u2013",  # â€“ -> â€“
    "\u00e2\u20ac\u201d": "\u2014",  # â€” -> â€”
    "\u00e2\u20ac\u0153": "\u201c",  # " -> "
    "\u00e2\u20ac\u009d": "\u201d",  # â„¢ï¿½ -> â€
    "\u00e2\u20ac\u02dc": "\u2018",  # ,
    'â€™':  -> ,
    'â€™': 
    "\u00e2\u20ac\u2122": "\u2019",  # â€™ -> â€™
    "\u00e2\u20ac\u00a6": "\u2026",  # â€¦ -> â€¦
    "\u00e2\u20a2": "\u00a2",        # Ã¢Â¢ -> Â¢
    "\u00e2\u20a6": "\u20a6",       # Ã¢Â¦ -> â‚¦ (rare)
    "\u00e2\u20a9": "\u20a9",       # Ã¢Â© -> â‚©
    "\u00c2\u00b0": "\u00b0",       # Â° -> Â°
    "\u00c2\u00aa": "\u00aa",       # Âª -> Âª
    "\u00c2\u00ba": "\u00ba",       # Âº -> Âº
    "\u00c2\u00b2": "\u00b2",       # Â² -> Â²
    "\u00c2\u00b3": "\u00b3",       # Â³ -> Â³
    "\u00c2\u00bd": "\u00bd",       # Â½ -> Â½
    "\u00c2\u00bc": "\u00bc",       # Â¼ -> Â¼
    "\u00c2\u00be": "\u00be",       # Â¾ -> Â¾
    "\u00c2\u00a3": "\u00a3",       # Â£ -> Â£
    "\u00c2\u00a5": "\u00a5",       # Â¥ -> Â¥
    "\u00c2\u00a7": "\u00a7",       # Â§ -> Â§
    "\u00c2\u00ab": "\u00ab",       # Â« -> Â«
    "\u00c2\u00bb": "\u00bb",       # Â» -> Â»
    "\u00c2\u00b7": "\u00b7",       # Â· -> Â·
    "\u00e2\u20a2": "\u20a2",       # Ã¢Â¢ -> Â¢ (alt)
    "\u00e2\u201a\u00ac": "\u20ac",  # â‚¬ -> â‚¬
    # double-encoded accent patterns (âˆšÂ° style)
    "\u00e2\u0088\u009a\u00c2\u00b0": "\u00e1",
    "\u00e2\u0088\u009a\u00c2\u00a2": "\u00e2",
    "\u00e2\u0088\u009a\u00c2\u00a9": "\u00e9",
    "\u00e2\u0088\u009a\u00c2\u00b9": "\u00ed",
    "\u00e2\u0088\u009a\u00c2\u00ba": "\u00fa",
    "\u00e2\u0088\u009a\u00c2\u00b3": "\u00f3",
    "\u00e2\u0088\u009a\u00c2\u00a3": "\u00e3",
    "\u00e2\u0088\u009a\u00c3\u0087": "\u00e7",
    "\u00e2\u0088\u009a\u00c3\u0089": "\u00c9",
    "\u00e2\u0088\u009a\u00c3\u0093": "\u00d3",
    "\u00e2\u0088\u009a\u00c3\u009c": "\u00dc",
    "\u00e2\u0088\u009a\u00c3\u00b1": "\u00f1",
    "\u00e2\u0088\u009a\u00c2\u00a7": "\u00a7",
    "\u00e2\u0088\u009a\u00c3\u00a7": "\u00e7",
    # variants where the stray Ã‚ is missing but bytes are the same pattern
    "\u00e2\u0088\u009a\u00b0": "\u00e1",
    "\u00e2\u0088\u009a\u00a2": "\u00e2",
    "\u00e2\u0088\u009a\u00a9": "\u00e9",
    "\u00e2\u0088\u009a\u00b9": "\u00ed",
    "\u00e2\u0088\u009a\u00ba": "\u00fa",
    "\u00e2\u0088\u009a\u00b3": "\u00f3",
    "\u00e2\u0088\u009a\u00a3": "\u00e3",
    "\u00e2\u0088\u009a\u00c7": "\u00e7",
    "\u00e2\u0088\u009a\u00c9": "\u00c9",
    "\u00e2\u0088\u009a\u00d3": "\u00d3",
    "\u00e2\u0088\u009a\u00dc": "\u00dc",
    "\u00e2\u0088\u009a\u00f1": "\u00f1",
    "\u00e2\u0088\u009a\u00a7": "\u00a7",
    "\u00e2\u0088\u009a\u00e7": "\u00e7",
    # variants without the stray Ã‚
    "\u221a\u00b0": "\u00e1",
    "\u221a\u00a2": "\u00e2",
    "\u221a\u00a9": "\u00e9",
    "\u221a\u00b9": "\u00ed",
    "\u221a\u00ba": "\u00fa",
    "\u221a\u00b3": "\u00f3",
    "\u221a\u00a3": "\u00e3",
    "\u221a\u00c7": "\u00e7",
    "\u221a\u00c9": "\u00c9",
    "\u221a\u00d3": "\u00d3",
    "\u221a\u00dc": "\u00dc",
    "\u221a\u00f1": "\u00f1",
    "\u221a\u00a7": "\u00a7",
    "\u221a\u00e7": "\u00e7",
    # common emoji/check/arrow fragments
    "\u00e2\u009c\u0094": "âœ”",
    "\u00e2\u009c\u0093": "âœ“",
    "\u00e2\u009c\u0085": "âœ…",
    "\u00e2\u009c\u00a8": "âœ¨",
    "\u00e2\u009d\u0097": "â€¼",
    "\u00e2\u009d\u0095": "â•",
    "\u00e2\u009d\u0093": "â“",
    "\u00e2\u009c\u00b6": "âœ¶",
    "\u00e2\u009e\u0098": "âœ˜",
    "\u00e2\u0086\u0092": "â†’",
    "\u00e2\u0086\u0093": "â†“",
    "\u00e2\u0086\u0091": "â†‘",
    "\u00e2\u0086\u0090": "â†",
    "\u00e2\u0086\u009b": "â†›",
    "\u00e2\u0086\u009c": "â†œ",
    "\u00e2\u0086\u009d": "â†",
    "\u00e2\u00ac\u0085": "â¬…",
    "\u00e2\u00ac\u0086": "â¬†",
    "\u00e2\u00ac\u0087": "â¬‡",
    "\u00e2\u0098\u00ba": "â˜º",
    "\u00f0\u009f\u0092\u00a1": "ðŸ’¡",
    "\u00f0\u009f\u0092\u00af": "ðŸ’¯",
    "\u00f0\u009f\u0093\u0097": "ðŸ“—",
    "\u00f0\u009f\u0093\u009a": "ðŸ“š",
    "\u00f0\u009f\u0093\u009d": "ðŸ“",
    "\u00f0\u009f\u0092\u00b8": "ðŸ’¸",
    "\u00f0\u009f\u0091\u008d": "ðŸ‘",
    "\u00f0\u009f\u0091\u008e": "ðŸ‘Ž",
    "\u00f0\u009f\u0091\u008f": "ðŸ‘",
    "\u00f0\u009f\u0098\u008a": "ðŸ˜Š",
    "\u00f0\u009f\u008e\u0089": "ðŸŽ‰",
    "\u00f0\u009f\u0093\u00a2": "ðŸ“¢",
    "\u00f0\u009f\u0094\u0096": "ðŸ”–",
    "\u00f0\u009f\u0094\u00a9": "ðŸ”©",
    "\u00f0\u009f\u0094\u00b9": "ðŸ”¹",
}

# Tokens for quick detection; include map keys plus a few extras
MOJIBAKE_TOKENS: tuple[str, ...] = tuple(
    set(
        list(MOJIBAKE_MAP.keys())
        + [
            "\u00c3\u00a0",
            "\u00c3\u00a2",
            "\u00c3\u00a3",
            "\u00c3\u00a7",
            "\u00c3\u00a9",
            "\u00c3\u00aa",
            "\u00c3\u00b5",
            "\u00e2\u20ac\u201d",
            "\u00e2\u20ac\u201c",
        ]
    )
)

ALLOWED_SUFFIXES: set[str] = {
    ".py",
    ".json",
    ".csv",
    ".md",
    ".txt",
    ".yml",
    ".yaml",
    ".ini",
    ".toml",
}

SKIP_DIRS: set[str] = {".git", ".ruff_cache", "__pycache__", ".vscode", ".qodo"}


def has_mojibake(text: str) -> bool:
    return any(tok in text for tok in MOJIBAKE_TOKENS)


def apply_fixes(text: str) -> str:
    fixed = text
    for bad, good in MOJIBAKE_MAP.items():
        if bad in fixed:
            fixed = fixed.replace(bad, good)
    return fixed


def iter_files(root: Path) -> Iterable[Path]:
    for path in root.rglob("*"):
        if path.is_dir():
            if path.name in SKIP_DIRS:
                continue
            continue
        if path.suffix.lower() not in ALLOWED_SUFFIXES:
            continue
        yield path


def process_file(path: Path) -> bool:
    try:
        try:
            text = path.read_text(encoding="utf-8")
        except UnicodeDecodeError:
            text = path.read_text(encoding="latin-1")
    except Exception as exc:
        print(f"[ERRO] Falha ao ler {path}: {exc}")
        return False

    if not has_mojibake(text):
        return False

    fixed = apply_fixes(text)
    if fixed == text:
        return False

    try:
        path.write_text(fixed, encoding="utf-8")
        print(f"[OK] Corrigido: {path}")
        return True
    except Exception as exc:
        print(f"[ERRO] Falha ao escrever {path}: {exc}")
        return False


def main() -> int:
    root = Path(sys.argv[1]) if len(sys.argv) > 1 else Path(__file__).resolve().parent
    if not root.exists():
        print(f"Caminho inexistente: {root}")
        return 1

    total = 0
    changed = 0
    for file_path in iter_files(root):
        total += 1
        if process_file(file_path):
            changed += 1

    print(f"\nArquivos analisados: {total}")
    print(f"Arquivos corrigidos: {changed}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
