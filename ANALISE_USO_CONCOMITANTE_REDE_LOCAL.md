# ğŸŒ ANÃLISE: Uso Concomitante em Rede Local - MÃºltiplos UsuÃ¡rios & MÃ¡quinas

## ğŸ“Š CENÃRIO ATUAL

**Arquitetura:**
- Sistema baseado em **CSV locais** (armazenamento)
- **Interface GUI** (CTk/Tkinter - mono-thread por sessÃ£o)
- Sem banco de dados
- Sem mecanismo de lock/sincronizaÃ§Ã£o

**Problemas Identificados:**

### ğŸ”´ **CRÃTICO: CorrupÃ§Ã£o de CSV**
```
MÃ¡quina A (Usuario JoÃ£o)  | MÃ¡quina B (Usuario Maria)
--------------------------|------------------------
1. LÃª historico_analises.csv
2. Processa 10 anÃ¡lises
3. Escreve CSV           | 1. LÃª historico_analises.csv
                         | 2. Processa 5 anÃ¡lises
                         | 3. Escreve CSV (SOBRESCREVE!)
4. Resultado: 5 anÃ¡lises de JoÃ£o perdidas âŒ
```

### ğŸ”´ **CRÃTICO: Race Condition nos Arquivos de Config**
```
Arquivo: banco/usuarios.csv

MÃ¡quina A              | MÃ¡quina B
-----------------------|-----------------------
1. LÃª usuarios.csv     |
2. Processa login      | 1. LÃª usuarios.csv
3. ...                 | 2. Altera senha user X
   ...                 | 3. Escreve usuarios.csv
4. Altera email user Y |
5. Escreve usuarios.csv (Sobrescreve dados de B!)
```

---

## ğŸ” ANÃLISE DETALHADA

### **1. Arquivos Afetados (Acesso CrÃ­tico)**

| Arquivo | Tipo | Acesso | Risco |
|---------|------|--------|-------|
| `logs/historico_analises.csv` | APPEND + UPDATE | R/W simultÃ¢neo | ğŸ”´ CRÃTICO |
| `banco/usuarios.csv` | AUTH + CRUD | R/W simultÃ¢neo | ğŸ”´ CRÃTICO |
| `banco/credenciais.csv` | AUTH | Leitura freq. | ğŸŸ¡ ALTO |
| `banco/exames_config.csv` | READ-ONLY* | Leitura freq. | ğŸŸ¢ BAIXO |
| `config/exams/*.json` | READ-ONLY | Leitura freq. | ğŸŸ¢ BAIXO |
| Outros CSVs | READ-ONLY | Leitura freq. | ğŸŸ¢ BAIXO |

*Config pode ser editado via UI, risco se simultÃ¢neo

---

## ğŸš¨ CENÃRIOS DE FALHA

### **CenÃ¡rio 1: HistÃ³rico de AnÃ¡lises (MAIS CRÃTICO)**

**CÃ³digo atual em `services/history_report.py`:**
```python
# OperaÃ§Ã£o 1: LÃª CSV (se existe)
if csv_path_obj.exists():
    df_existente = pd.read_csv(csv_path_obj, sep=";", encoding="utf-8")

# OperaÃ§Ã£o 2: Adiciona novas linhas
df_hist = pd.DataFrame(linhas)

# OperaÃ§Ã£o 3: Valida colunas
if colunas_existentes != colunas_esperadas:
    # ... modifica df_existente

# OperaÃ§Ã£o 4: ESCREVE (PERIGOSO!)
df_hist.to_csv(caminho_csv, sep=";", index=False, mode="a", ...)
```

**Problema:** Entre Leitura (Op 1) e Escrita (Op 4), outra mÃ¡quina pode ter modificado o arquivo!

**Resultado:**
- MÃ¡quina A lÃª: [linhas 1-100]
- MÃ¡quina B lÃª: [linhas 1-100]
- MÃ¡quina A adiciona: +50 linhas â†’ [1-150]
- MÃ¡quina B adiciona: +30 linhas â†’ [1-130] âŒ Perde 20 linhas de A!

---

### **CenÃ¡rio 2: AutenticaÃ§Ã£o de UsuÃ¡rios**

**CÃ³digo em `core/authentication/user_manager.py`:**
```python
def _carregar_usuarios(self):
    with open(self.csv_path, "r", encoding="utf-8") as file:
        reader = csv.DictReader(file)
        # Processa usuÃ¡rios

def _salvar_usuarios(self, usuarios):
    with open(self.csv_path, "w", newline="", encoding="utf-8") as file:
        # Escreve todos de novo
```

**Problema:** Abertura exclusiva de arquivo, sem compartilhamento seguro

**Resultado:**
- MÃ¡quina A: Altera senha do usuÃ¡rio X
- MÃ¡quina B: Altera email do usuÃ¡rio Y
- Se MÃ¡quina B escrever DEPOIS, dados de A sÃ£o perdidos âŒ

---

### **CenÃ¡rio 3: AtualizaÃ§Ã£o de Status GAL**

**CÃ³digo em `services/history_report.py` - `atualizar_status_gal()`:**
```python
df = pd.read_csv(csv_path_obj, ...)
# ... modifica df ...
df.to_csv(csv_path_obj, ...)  # SOBRESCREVE!
```

**Problema:** Outro usuÃ¡rio pode estar adicionando anÃ¡lises ENQUANTO atualiza status

**Resultado:**
- MÃ¡quina A: LÃª CSV (100 linhas)
- MÃ¡quina B: Adiciona 10 linhas â†’ 110 linhas
- MÃ¡quina A: Atualiza status â†’ Escreve 100 linhas (PERDE 10!) âŒ

---

## ğŸ” SOLUÃ‡Ã•ES RECOMENDADAS

### **OpÃ§Ã£o 1: File-Based Locking (RÃ¡pido, Baixo Custo)**

**Vantagens:**
- âœ… Sem dependÃªncia externa
- âœ… Funciona em rede local
- âœ… FÃ¡cil implementar em CSV

**Desvantagens:**
- âŒ Pode travar em case de crash
- âŒ Performance degradada
- âŒ NÃ£o escala para mÃºltiplos servidores

**ImplementaÃ§Ã£o:**
```python
import fcntl
import time
from pathlib import Path

class CsvFileLock:
    def __init__(self, csv_path, timeout=30):
        self.csv_path = csv_path
        self.lock_path = Path(csv_path).with_suffix('.lock')
        self.timeout = timeout
    
    def __enter__(self):
        start = time.time()
        while self.lock_path.exists():
            if time.time() - start > self.timeout:
                raise TimeoutError(f"Lock timeout em {self.csv_path}")
            time.sleep(0.1)
        self.lock_path.touch()
        return self
    
    def __exit__(self, *args):
        self.lock_path.unlink(missing_ok=True)

# Uso:
with CsvFileLock("logs/historico_analises.csv"):
    df = pd.read_csv(...)
    # ... processa ...
    df.to_csv(...)  # Seguro!
```

**LimitaÃ§Ã£o:** Funciona bem em rede local NFS/SMB, mas pode ter delays

---

### **OpÃ§Ã£o 2: SQLite com Lock (RECOMENDADO para Rede Local)**

**Vantagens:**
- âœ… Lock automÃ¡tico built-in
- âœ… ACID transactions
- âœ… CompartilhÃ¡vel em rede local
- âœ… Sem servidor necessÃ¡rio
- âœ… LÃª/escreve em arquivo Ãºnico
- âœ… FÃ¡cil migraÃ§Ã£o de CSV

**Desvantagens:**
- âŒ Requer refatoraÃ§Ã£o de cÃ³digo
- âŒ Performance degrada com writes simultÃ¢neos
- âŒ Suporte limitado a NFS remoto

**ImplementaÃ§Ã£o BÃ¡sica:**
```python
import sqlite3
from contextlib import contextmanager

@contextmanager
def obter_conexao_db(db_path="banco/integragal.db"):
    conn = sqlite3.connect(db_path, timeout=30, check_same_thread=False)
    conn.execute("PRAGMA journal_mode=WAL")  # Write-Ahead Logging
    try:
        yield conn
    finally:
        conn.close()

# Tabela: historico_analises
# CREATE TABLE historico (
#     id_registro TEXT PRIMARY KEY,
#     data_hora_analise TIMESTAMP,
#     usuario_analise TEXT,
#     exame TEXT,
#     status_gal TEXT,
#     data_hora_envio TIMESTAMP NULL,
#     usuario_envio TEXT NULL,
#     sucesso_envio BOOLEAN NULL,
#     detalhes_envio TEXT,
#     criado_em TIMESTAMP,
#     atualizado_em TIMESTAMP
# );

# InserÃ§Ã£o segura:
with obter_conexao_db() as conn:
    cursor = conn.cursor()
    cursor.execute(
        """INSERT INTO historico 
           (id_registro, data_hora_analise, usuario_analise, ...)
           VALUES (?, ?, ?, ...)""",
        (uuid.uuid4(), timestamp, usuario, ...)
    )
    conn.commit()  # Lock liberado automaticamente

# AtualizaÃ§Ã£o segura:
with obter_conexao_db() as conn:
    cursor = conn.cursor()
    cursor.execute(
        """UPDATE historico 
           SET status_gal=?, data_hora_envio=?, usuario_envio=?, sucesso_envio=?
           WHERE id_registro=?""",
        ("enviado", timestamp, usuario, True, id_registro)
    )
    conn.commit()
```

---

### **OpÃ§Ã£o 3: PostgreSQL/MySQL em Servidor (MELHOR ESCALABILIDADE)**

**Vantagens:**
- âœ… ConcorrÃªncia ilimitada
- âœ… ACID completo
- âœ… EscalÃ¡vel (multi-servidor)
- âœ… Backup centralizado
- âœ… ReplicaÃ§Ã£o possÃ­vel

**Desvantagens:**
- âŒ Requer servidor rodando
- âŒ Complexidade aumentada
- âŒ Custo de infraestrutura
- âŒ Setup inicial complexo

---

## ğŸ“ˆ COMPARAÃ‡ÃƒO DE SOLUÃ‡Ã•ES

| Aspecto | CSV atual | File Lock | SQLite | PostgreSQL |
|--------|-----------|-----------|--------|------------|
| **ConcorrÃªncia** | âŒ Nenhuma | ğŸŸ¡ BÃ¡sica | âœ… Boa | âœ… Excelente |
| **Rede Local** | âš ï¸ Perigoso | âœ… Funciona | âœ… Funciona | âœ… Funciona |
| **MÃºltiplas mÃ¡quinas** | âŒ NÃ£o | ğŸŸ¡ Lento | âœ… Sim | âœ… Sim |
| **Integridade dados** | âŒ Fraca | ğŸŸ¡ MÃ©dia | âœ… Forte | âœ… Forte |
| **Performance** | âœ… RÃ¡pido | ğŸŸ¡ MÃ©dio | âœ… RÃ¡pido | âœ… RÃ¡pido |
| **ImplementaÃ§Ã£o** | âœ… Trivial | ğŸŸ¡ FÃ¡cil | âœ… MÃ©dio | âŒ Complexo |
| **ManutenÃ§Ã£o** | âœ… Nenhuma | ğŸŸ¡ Pouca | âœ… Pouca | âŒ Muita |
| **Escalabilidade** | âŒ NÃ£o | âŒ NÃ£o | ğŸŸ¡ Limitada | âœ… Sim |
| **Custo** | âœ… Zero | âœ… Zero | âœ… Zero | ğŸŸ¡ Servidor |

---

## ğŸ¯ RECOMENDAÃ‡ÃƒO PARA SEU CASO

### **CURTO PRAZO (Imediato - PrÃ³xima 1-2 semanas)**

**Use: File-Based Locking + CSV**

Motivo:
- âœ… ImplementaÃ§Ã£o rÃ¡pida (< 2h)
- âœ… Zero dependÃªncias externas
- âœ… Funciona com setup atual
- âœ… Suficiente para rede local ~5-10 usuÃ¡rios

```python
# Criar arquivo: services/csv_lock.py
import fcntl
import time
from pathlib import Path
from contextlib import contextmanager
import logging

logger = logging.getLogger(__name__)

@contextmanager
def csv_lock(filepath: str, timeout: int = 30):
    """
    Context manager para lock seguro em CSV em rede local.
    Usa arquivo .lock para sincronizaÃ§Ã£o.
    """
    lock_file = Path(filepath).with_suffix('.lock')
    acquired_at = time.time()
    
    # Aguarda lock ficar disponÃ­vel
    while lock_file.exists():
        if time.time() - acquired_at > timeout:
            raise TimeoutError(f"Timeout aguardando lock em {filepath}")
        time.sleep(0.05)
    
    try:
        # Adquire lock
        lock_file.touch()
        logger.info(f"Lock adquirido: {filepath}")
        yield
    finally:
        # Libera lock
        lock_file.unlink(missing_ok=True)
        logger.info(f"Lock liberado: {filepath}")

# Uso em history_report.py:
from services.csv_lock import csv_lock

def gerar_historico_csv(...):
    with csv_lock(caminho_csv):
        df_existente = pd.read_csv(caminho_csv, ...)
        # ... processa ...
        df_hist.to_csv(caminho_csv, ...)

def atualizar_status_gal(...):
    with csv_lock(csv_path):
        df = pd.read_csv(csv_path, ...)
        # ... atualiza ...
        df.to_csv(csv_path, ...)
```

**Impacto:**
- âœ… Elimina corrupÃ§Ã£o de dados
- âœ… OperaÃ§Ãµes atÃ´micas
- âš ï¸ Performance ok para atÃ© 50 operaÃ§Ãµes/minuto

---

### **MÃ‰DIO PRAZO (PrÃ³ximo mÃªs)**

**Use: SQLite com WAL mode**

Motivo:
- âœ… Melhor performance que file locks
- âœ… TransaÃ§Ãµes ACID
- âœ… Suporta mÃºltiplas conexÃµes simultÃ¢neas

```python
# Exemplo: historico_analises em SQLite
# Create table uma vez:
# CREATE TABLE historico_analises (
#     id_registro TEXT PRIMARY KEY,
#     data_hora_analise TEXT,
#     usuario_analise TEXT,
#     exame TEXT,
#     status_gal TEXT,
#     data_hora_envio TEXT,
#     usuario_envio TEXT,
#     sucesso_envio INTEGER,
#     detalhes_envio TEXT,
#     criado_em TEXT,
#     atualizado_em TEXT
# );

import sqlite3
from contextlib import contextmanager

@contextmanager
def db_context(db_path: str = "banco/integragal.db"):
    conn = sqlite3.connect(db_path, timeout=30)
    conn.execute("PRAGMA journal_mode=WAL")  # Ativa WAL
    try:
        yield conn
    finally:
        conn.close()

# InserÃ§Ã£o:
with db_context() as conn:
    cursor = conn.cursor()
    cursor.execute(
        """INSERT INTO historico_analises 
           (id_registro, data_hora_analise, usuario_analise, exame, status_gal, criado_em, atualizado_em)
           VALUES (?, ?, ?, ?, ?, ?, ?)""",
        (id_reg, dt_analise, usuario, exame, "nÃ£o enviado", timestamp, timestamp)
    )
    conn.commit()

# AtualizaÃ§Ã£o:
def atualizar_status_gal(id_registros, sucesso, usuario_envio, detalhes):
    with db_context() as conn:
        cursor = conn.cursor()
        novo_status = "enviado" if sucesso else "falha no envio"
        
        for id_reg in id_registros:
            cursor.execute(
                """UPDATE historico_analises
                   SET status_gal=?, data_hora_envio=?, usuario_envio=?, sucesso_envio=?, detalhes_envio=?, atualizado_em=?
                   WHERE id_registro=?""",
                (novo_status, datetime.now(), usuario_envio, sucesso, detalhes, datetime.now(), id_reg)
            )
        conn.commit()
        return {"registros_atualizados": cursor.rowcount}
```

---

### **LONGO PRAZO (PrÃ³ximo trimestre)**

**Use: PostgreSQL em servidor Linux**

Motivo:
- âœ… Escalabilidade ilimitada
- âœ… Suporte a 1000s de usuÃ¡rios simultÃ¢neos
- âœ… Backup/replicaÃ§Ã£o automÃ¡tica
- âœ… Possibilidade de cloud migration

---

## âš ï¸ PROBLEMAS ADICIONAIS IDENTIFICADOS

### **1. HistÃ³rico de AnÃ¡lises**
- âŒ Sem proteÃ§Ã£o contra corrupÃ§Ã£o simultÃ¢nea
- âœ… **SoluÃ§Ã£o:** CSV Lock + TransaÃ§Ãµes

### **2. AutenticaÃ§Ã£o (usuarios.csv)**
- âŒ Sem proteÃ§Ã£o contra race condition
- âœ… **SoluÃ§Ã£o:** CSV Lock + ValidaÃ§Ã£o

### **3. AtualizaÃ§Ã£o de Status GAL**
- âŒ Sem versioning - pode sobrescrever alteraÃ§Ãµes recentes
- âœ… **SoluÃ§Ã£o:** Usar UPDATE com WHERE em vez de sobrescrever tudo

### **4. ConfiguraÃ§Ãµes (exames_config.csv)**
- âŒ Se editado via UI enquanto outra mÃ¡quina lÃª
- âœ… **SoluÃ§Ã£o:** CSV Lock + Reload em memÃ³ria

---

## ğŸ“‹ CHECKLIST DE IMPLEMENTAÃ‡ÃƒO

### Fase 1: File Locks (IMEDIATO)
- [ ] Criar `services/csv_lock.py`
- [ ] Atualizar `gerar_historico_csv()` com lock
- [ ] Atualizar `atualizar_status_gal()` com lock
- [ ] Atualizar `_salvar_usuarios()` com lock
- [ ] Atualizar outros CSV writes com lock
- [ ] Testes: 2 mÃ¡quinas simultÃ¢neas

### Fase 2: SQLite (PrÃ³ximo mÃªs)
- [ ] Design schema SQLite
- [ ] Criar `services/db_manager.py`
- [ ] Migrar `historico_analises` para SQLite
- [ ] Migrar `usuarios` para SQLite
- [ ] Testes de carga (100 ops/min)

### Fase 3: PostgreSQL (Futuro)
- [ ] Setup servidor PostgreSQL
- [ ] Design completo de BD
- [ ] MigraÃ§Ã£o dados de SQLite
- [ ] Testes de replicaÃ§Ã£o

---

## ğŸ”§ IMPLEMENTAÃ‡ÃƒO RÃPIDA (FILE LOCK)

**Tempo estimado:** ~1-2 horas

```python
# services/csv_lock.py
import time
from pathlib import Path
from contextlib import contextmanager
import logging

logger = logging.getLogger(__name__)

@contextmanager
def csv_lock(filepath: str, timeout: int = 30, lock_suffix: str = ".lock"):
    """
    Lock seguro para CSV em rede local.
    
    Uso:
        with csv_lock("logs/historico.csv"):
            df = pd.read_csv("logs/historico.csv")
            # ... processa ...
            df.to_csv("logs/historico.csv")
    """
    lock_path = Path(filepath).with_suffix(lock_suffix)
    start_time = time.time()
    
    # Aguarda lock
    while lock_path.exists():
        if time.time() - start_time > timeout:
            raise TimeoutError(f"Timeout esperando lock para {filepath}")
        time.sleep(0.05)
    
    try:
        lock_path.touch()
        logger.info(f"âœ… Lock: {Path(filepath).name}")
        yield
    finally:
        lock_path.unlink(missing_ok=True)
        logger.info(f"ğŸ”“ Liberado: {Path(filepath).name}")
```

---

**ConclusÃ£o:**
Para **rede local com 3-10 usuÃ¡rios simultÃ¢neos**, use **File Locks + CSV** (curto prazo).
Planeje migraÃ§Ã£o para **SQLite** quando usar chegar a **20+ usuÃ¡rios simultÃ¢neos**.
